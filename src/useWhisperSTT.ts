import { useState, useRef, useCallback, useEffect } from 'react';
import { STTResult, ModelInfo, WorkerMessage, WorkerResponse } from './types';

interface UseWhisperSTTReturn {
  isModelLoading: boolean;
  isModelReady: boolean;
  isProcessing: boolean;
  error: string | null;
  loadingProgress: number;
  modelInfo: ModelInfo | null;
  transcribeAudio: (audioBlob: Blob, timestamp: number, language?: string, returnTimestamps?: boolean) => Promise<STTResult>;
  initializeModel: () => Promise<void>;
  clearQueue: () => void;
  waitForQueueCompletion: () => Promise<void>;
  getQueueLength: () => number;
}

interface QueueItem {
  audioBlob: Blob;
  timestamp: number;
  language: string;
  returnTimestamps: boolean;
  resolve: (result: STTResult) => void;
  reject: (error: Error) => void;
}

export const useWhisperSTT = (): UseWhisperSTTReturn => {
  const [isModelLoading, setIsModelLoading] = useState(false);
  const [isModelReady, setIsModelReady] = useState(false);
  const [isProcessing, setIsProcessing] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const [loadingProgress, setLoadingProgress] = useState<number>(0);
  const [modelInfo, setModelInfo] = useState<ModelInfo | null>(null);
  
  const workerRef = useRef<Worker | null>(null);
  const processingQueueRef = useRef<QueueItem[]>([]);
  const isProcessingRef = useRef(false);

  const initializeModel = useCallback(async () => {
    if (workerRef.current || isModelLoading) return;
    
    try {
      setIsModelLoading(true);
      setError(null);
      setLoadingProgress(0);
      
      console.log('Web Workerì™€ Whisper Turbo ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...');
      
      // Web Worker ìƒì„± - npmìœ¼ë¡œ ì„¤ì¹˜ëœ transformers.js ì‚¬ìš©
      const worker = new Worker(new URL('./workers/whisper-worker.ts', import.meta.url), {
        type: 'module'
      });
      
      worker.onmessage = (e: MessageEvent<WorkerResponse>) => {
        const { type, status, message, progress, text, timestamp } = e.data;
        
        switch (type) {
          case 'loading':
            if (status === 'downloading') {
              setLoadingProgress(progress || 0);
              console.log(message);
            } else if (status === 'ready') {
              setIsModelReady(true);
              setIsModelLoading(false);
              setLoadingProgress(100);
              setModelInfo({ description: 'Whisper Large V3 Turbo' });
              console.log(message);
            }
            break;
            
          case 'transcribing':
            setIsProcessing(true);
            console.log(message);
            break;
            
          case 'result':
            if (text !== undefined && timestamp !== undefined) {
              handleTranscriptionResult({ text, timestamp });
            }
            break;
            
          case 'error':
            console.error('Worker error:', message);
            if (timestamp) {
              handleTranscriptionError(new Error(message || 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'), timestamp);
            } else {
              setError(message || 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜');
              setIsModelLoading(false);
            }
            break;
          default:
            // ignore unknown message types
            break;
        }
      };

      worker.onerror = (error: ErrorEvent) => {
        console.error('Worker error details:', {
          message: error.message,
          filename: error.filename,
          lineno: error.lineno,
          colno: error.colno,
          error: error.error
        });
        setError(`Web Worker ì˜¤ë¥˜: ${error.message} (in ${error.filename}:${error.lineno})`);
        setIsModelLoading(false);
      };
      
      workerRef.current = worker;
      
      // Whisper Turbo ëª¨ë¸ ë¡œë“œ ìš”ì²­
      const loadMessage: WorkerMessage = {
        type: 'load-model',
        model: 'onnx-community/whisper-large-v3-turbo_timestamped'
      };
      worker.postMessage(loadMessage);
      
    } catch (err) {
      console.error('Worker ì´ˆê¸°í™” ì‹¤íŒ¨:', err);
      const errorMessage = err instanceof Error ? err.message : 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜';
      setError('Worker ì´ˆê¸°í™” ì‹¤íŒ¨: ' + errorMessage);
      setIsModelLoading(false);
    }
  }, []);

  const audioBufferFromBlob = useCallback(async (blob: Blob): Promise<Float32Array> => {
    const arrayBuffer = await blob.arrayBuffer();
    const audioContext = new AudioContext({ sampleRate: 16000 });
    const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
    
    let audioData;
    if (audioBuffer.numberOfChannels === 1) {
      audioData = audioBuffer.getChannelData(0);
    } else {
      const leftChannel = audioBuffer.getChannelData(0);
      const rightChannel = audioBuffer.getChannelData(1);
      audioData = new Float32Array(leftChannel.length);
      for (let i = 0; i < leftChannel.length; i++) {
        audioData[i] = (leftChannel[i] + rightChannel[i]) / 2;
      }
    }
    
    if (audioBuffer.sampleRate !== 16000) {
      const resampledLength = Math.round(audioData.length * 16000 / audioBuffer.sampleRate);
      const resampledData = new Float32Array(resampledLength);
      const ratio = audioData.length / resampledLength;
      
      for (let i = 0; i < resampledLength; i++) {
        const srcIndex = Math.round(i * ratio);
        resampledData[i] = audioData[Math.min(srcIndex, audioData.length - 1)];
      }
      
      audioData = resampledData;
    }
    
    await audioContext.close();
    return audioData;
  }, []);

  const processAudioQueue = useCallback(async () => {
    if (isProcessingRef.current || processingQueueRef.current.length === 0 || !workerRef.current) {
      return;
    }

    isProcessingRef.current = true;
    setIsProcessing(true);

    while (processingQueueRef.current.length > 0 && workerRef.current) {
      const queueItem = processingQueueRef.current[0];
      const { audioBlob, timestamp, language, returnTimestamps, resolve, reject } = queueItem;

      try {
        console.log('STT ì²˜ë¦¬ ì‹œì‘:', new Date(timestamp).toLocaleTimeString());
        
        const audioData = await audioBufferFromBlob(audioBlob);
        
        if (audioData.length === 0) {
          resolve({ text: '', timestamp });
          processingQueueRef.current.shift();
          continue;
        }

        // Workerë¡œ transcription ìš”ì²­ ì „ì†¡
        const transcribeMessage: WorkerMessage = {
          type: 'transcribe',
          data: {
            audioData: audioData,
            options: {
              language: language,
              task: 'transcribe',
              chunk_length_s: 30,
              stride_length_s: 5,
              return_timestamps: returnTimestamps,
              timestamp: timestamp
            }
          }
        };
        workerRef.current.postMessage(transcribeMessage);

        break; // Workerì—ì„œ ê²°ê³¼ë¥¼ ê¸°ë‹¤ë¦¬ê¸° ìœ„í•´ ë£¨í”„ ì¤‘ë‹¨
        
      } catch (err) {
        console.error('STT ì²˜ë¦¬ ì˜¤ë¥˜:', err);
        const error = err instanceof Error ? err : new Error('ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜');
        reject(error);
        processingQueueRef.current.shift();
      }
    }

    if (processingQueueRef.current.length === 0) {
      isProcessingRef.current = false;
      setIsProcessing(false);
    }
  }, [audioBufferFromBlob]);

  const handleTranscriptionResult = useCallback(({ text, timestamp }: { text: string; timestamp: number }) => {
    const queueItem = processingQueueRef.current.find(item => item.timestamp === timestamp);
    if (queueItem) {
      queueItem.resolve({ text, timestamp });
      processingQueueRef.current = processingQueueRef.current.filter(item => item.timestamp !== timestamp);
    }
    
    // í˜„ì¬ ì•„ì´í…œ ì²˜ë¦¬ê°€ ëë‚¬ìœ¼ë¯€ë¡œ ë‹¤ìŒ ì•„ì´í…œ ì²˜ë¦¬ë¥¼ ìœ„í•´ í”Œë˜ê·¸ë¥¼ í•´ì œ
    isProcessingRef.current = false;
    
    if (processingQueueRef.current.length === 0) {
      setIsProcessing(false);
    } else {
      // ë‚¨ì€ í•­ëª©ì´ ìˆìœ¼ë©´ ì¦‰ì‹œ ë‹¤ìŒ í•­ëª© ì²˜ë¦¬
      processAudioQueue();
    }
  }, [processAudioQueue]);

  const handleTranscriptionError = useCallback((error: Error, timestamp: number) => {
    const queueItem = processingQueueRef.current.find(item => item.timestamp === timestamp);
    if (queueItem) {
      queueItem.reject(error);
      processingQueueRef.current = processingQueueRef.current.filter(item => item.timestamp !== timestamp);
    }
    
    // ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ ë‹¤ìŒ ì‘ì—…ì„ ì§„í–‰í•  ìˆ˜ ìˆë„ë¡ í”Œë˜ê·¸ í•´ì œ
    isProcessingRef.current = false;
    
    if (processingQueueRef.current.length === 0) {
      setIsProcessing(false);
    } else {
      processAudioQueue();
    }
  }, [processAudioQueue]);

  const getQueueLength = useCallback(() => {
    return processingQueueRef.current.length;
  }, []);

  const waitForQueueCompletion = useCallback((): Promise<void> => {
    return new Promise((resolve) => {
      const checkQueue = () => {
        if (processingQueueRef.current.length === 0 && !isProcessingRef.current) {
          console.log('âœ… STT í ì™„ë£Œ ëŒ€ê¸° ì¢…ë£Œ');
          resolve();
        } else {
          console.log('â³ STT í ëŒ€ê¸° ì¤‘:', processingQueueRef.current.length, 'ê°œ í•­ëª© ë‚¨ìŒ');
          setTimeout(checkQueue, 100);
        }
      };
      checkQueue();
    });
  }, []);

  const clearQueue = useCallback(() => {
    console.log('ğŸ§¹ STT í í´ë¦¬ì–´:', processingQueueRef.current.length, 'ê°œ í•­ëª© ì œê±°');
    
    // ëŒ€ê¸° ì¤‘ì¸ ëª¨ë“  ìš”ì²­ì„ ê±°ë¶€
    processingQueueRef.current.forEach(item => {
      item.reject(new Error('ì–¸ì–´ ë³€ê²½ìœ¼ë¡œ ì¸í•œ í í´ë¦¬ì–´'));
    });
    
    // í ë¹„ìš°ê¸°
    processingQueueRef.current = [];
    
    // ì²˜ë¦¬ ìƒíƒœ ì´ˆê¸°í™”
    isProcessingRef.current = false;
    setIsProcessing(false);
  }, []);

  const transcribeAudio = useCallback(async (audioBlob: Blob, timestamp: number, language: string = 'korean', returnTimestamps: boolean = false): Promise<STTResult> => {
    if (!workerRef.current || !isModelReady) {
      throw new Error('ëª¨ë¸ì´ ì•„ì§ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤');
    }

    return new Promise<STTResult>((resolve, reject) => {
      processingQueueRef.current.push({
        audioBlob,
        timestamp,
        language,
        returnTimestamps,
        resolve,
        reject
      });
      
      processAudioQueue();
    });
  }, [processAudioQueue, isModelReady]);

  useEffect(() => {
    initializeModel();
    
    // cleanup: component unmount ì‹œ worker ì •ë¦¬
    return () => {
      if (workerRef.current) {
        workerRef.current.terminate();
        workerRef.current = null;
      }
    };
  }, [initializeModel]);

  return {
    isModelLoading,
    isModelReady,
    isProcessing,
    error,
    loadingProgress,
    modelInfo,
    transcribeAudio,
    initializeModel,
    clearQueue,
    waitForQueueCompletion,
    getQueueLength
  };
};