import { useState, useRef, useCallback, useEffect } from 'react';
import { MicVAD } from '@ricky0123/vad-web';
import { VADStatus } from './types';

interface UseVADRecordingReturn {
  isRecording: boolean;
  error: string | null;
  vadStatus: VADStatus;
  segmentCount: number;
  audioLevel: number;
  startRecording: () => Promise<void>;
  stopRecording: () => void;
  pauseRecording: () => void;
  resumeRecording: () => void;
}

type OnAudioSegmentCallback = (audioBlob: Blob, timestamp: number) => Promise<void>;

export const useVADRecording = (onAudioSegment: OnAudioSegmentCallback): UseVADRecordingReturn => {
  const [isRecording, setIsRecording] = useState<boolean>(false);
  const [error, setError] = useState<string | null>(null);
  const [vadStatus, setVadStatus] = useState<VADStatus>('idle'); // idle, listening, speaking, processing
  const [audioLevel, setAudioLevel] = useState<number>(0);
  
  const vadRef = useRef<any>(null); // MicVAD íƒ€ì…ì´ ë³µì¡í•˜ë¯€ë¡œ any ì‚¬ìš©
  const streamRef = useRef<MediaStream | null>(null);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const chunksRef = useRef<Blob[]>([]);
  const segmentStartTimeRef = useRef<number>(0);
  const segmentCountRef = useRef<number>(0);
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const animationFrameRef = useRef<number | null>(null);

  // ì˜¤ë””ì˜¤ ë ˆë²¨ ë¶„ì„ ì‹œì‘ (ì„±ëŠ¥ ìµœì í™” ë²„ì „)
  const startAudioLevelAnalysis = useCallback((stream: MediaStream) => {
    try {
      const audioContext = new AudioContext();
      const source = audioContext.createMediaStreamSource(stream);
      const analyser = audioContext.createAnalyser();
      
      // ì„±ëŠ¥ ìµœì í™”: FFT í¬ê¸° ê°ì†Œ, ìŠ¤ë¬´ë”© ì¡°ì •
      analyser.fftSize = 128; // 256ì—ì„œ 128ë¡œ ê°ì†Œ (ê³„ì‚° ë¶€í•˜ ì ˆë°˜)
      analyser.smoothingTimeConstant = 0.9; // ë” ë¶€ë“œëŸ¬ìš´ ë³€í™”
      source.connect(analyser);
      
      audioContextRef.current = audioContext;
      analyserRef.current = analyser;
      
      const dataArray = new Uint8Array(analyser.frequencyBinCount);
      
      const updateAudioLevel = () => {
        if (!analyserRef.current) return;
        
        analyserRef.current.getByteFrequencyData(dataArray);
        const average = dataArray.reduce((acc, value) => acc + value, 0) / dataArray.length;
        const normalizedLevel = Math.min(100, (average / 128) * 100);
        const roundedLevel = Math.round(normalizedLevel);
        
        // ë ˆë²¨ì´ ì‹¤ì œë¡œ ë³€ê²½ëœ ê²½ìš°ë§Œ ìƒíƒœ ì—…ë°ì´íŠ¸ (ë¶ˆí•„ìš”í•œ ë¦¬ë Œë”ë§ ë°©ì§€)
        setAudioLevel(prev => prev !== roundedLevel ? roundedLevel : prev);
        
        // 60fps ëŒ€ì‹  30fpsë¡œ ì œí•œí•˜ì—¬ ì„±ëŠ¥ í–¥ìƒ
        setTimeout(() => {
          animationFrameRef.current = requestAnimationFrame(updateAudioLevel);
        }, 33); // ~30fps
      };
      
      updateAudioLevel();
      
    } catch (error) {
      console.error('ì˜¤ë””ì˜¤ ë ˆë²¨ ë¶„ì„ ì´ˆê¸°í™” ì‹¤íŒ¨:', error);
    }
  }, []);

  // ì˜¤ë””ì˜¤ ë ˆë²¨ ë¶„ì„ ì¤‘ì§€
  const stopAudioLevelAnalysis = useCallback(() => {
    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current);
      animationFrameRef.current = null;
    }
    
    if (audioContextRef.current) {
      audioContextRef.current.close();
      audioContextRef.current = null;
    }
    
    analyserRef.current = null;
    setAudioLevel(0);
  }, []);

  // VAD ì´ˆê¸°í™”
  const initializeVAD = useCallback(async (stream: MediaStream): Promise<void> => {
    try {
      console.log('VAD ì´ˆê¸°í™” ì¤‘...');
      
      const vad = await MicVAD.new({
        onSpeechStart: () => {
          console.log('ğŸ¤ ìŒì„± ì‹œì‘ ê°ì§€');
          setVadStatus('speaking');
          
          // ìŒì„±ì´ ì‹œì‘ë˜ë©´ ë…¹ìŒ ì‹œì‘
          if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'recording') {
            chunksRef.current = [];
            segmentStartTimeRef.current = Date.now();
            segmentCountRef.current++;
            
            console.log(`ì„¸ê·¸ë¨¼íŠ¸ #${segmentCountRef.current} ë…¹ìŒ ì‹œì‘`);
            mediaRecorderRef.current.start();
          }
        },
        
        onSpeechEnd: (audio: Float32Array) => {
          console.log('ğŸ”‡ ìŒì„± ì¢…ë£Œ ê°ì§€');
          setVadStatus('processing');
          
          // ìŒì„±ì´ ëë‚˜ë©´ ë…¹ìŒ ì¤‘ì§€ ë° STT ì²˜ë¦¬
          if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
            console.log(`ì„¸ê·¸ë¨¼íŠ¸ #${segmentCountRef.current} ë…¹ìŒ ì¢…ë£Œ`);
            mediaRecorderRef.current.stop();
          }
        },
        
        onVADMisfire: () => {
          console.log('âš ï¸ VAD ì˜¤íƒì§€');
          setVadStatus('listening');
        },
        
        // VAD ì„¤ì •
        positiveSpeechThreshold: 0.5,  // ìŒì„± ê°ì§€ ë¯¼ê°ë„
        negativeSpeechThreshold: 0.35, // ìŒì„± ì¢…ë£Œ ë¯¼ê°ë„
        redemptionFrames: 8,           // ìŒì„± ë³µêµ¬ í”„ë ˆì„
        frameSamples: 1536,            // í”„ë ˆì„ í¬ê¸°
        preSpeechPadFrames: 1,         // ìŒì„± ì‹œì‘ ì „ íŒ¨ë”©
        minSpeechFrames: 3,            // ìµœì†Œ ìŒì„± í”„ë ˆì„
        stream: stream
      });
      
      vadRef.current = vad;
      console.log('âœ… VAD ì´ˆê¸°í™” ì™„ë£Œ');
      
    } catch (err) {
      console.error('VAD ì´ˆê¸°í™” ì‹¤íŒ¨:', err);
      const errorMessage = err instanceof Error ? err.message : 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜';
      setError('ìŒì„± ê°ì§€ ì´ˆê¸°í™” ì‹¤íŒ¨: ' + errorMessage);
    }
  }, []);

  const startRecording = useCallback(async () => {
    try {
      setError(null);
      setVadStatus('listening');
      segmentCountRef.current = 0;
      
      // ë§ˆì´í¬ ìŠ¤íŠ¸ë¦¼ íšë“
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          sampleRate: 16000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        }
      });

      streamRef.current = stream;
      
      // MediaRecorder ì„¤ì •
      const mediaRecorder = new MediaRecorder(stream, {
        mimeType: 'audio/webm;codecs=opus'
      });
      
      mediaRecorderRef.current = mediaRecorder;

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          chunksRef.current.push(event.data);
        }
      };

      mediaRecorder.onstop = async () => {
        if (chunksRef.current.length > 0) {
          const blob = new Blob(chunksRef.current, { type: 'audio/webm' });
          const segmentStartTime = segmentStartTimeRef.current;
          
          console.log(`ì„¸ê·¸ë¨¼íŠ¸ #${segmentCountRef.current} STT ì²˜ë¦¬ ìš”ì²­`);
          
          // STT ì²˜ë¦¬
          await onAudioSegment(blob, segmentStartTime);
          
          chunksRef.current = [];
        }
        
        // STT ì²˜ë¦¬ ì™„ë£Œ í›„ ë‹¤ì‹œ ëŒ€ê¸° ìƒíƒœë¡œ
        setVadStatus('listening');
      };

      // VAD ì´ˆê¸°í™” ë° ì‹œì‘
      await initializeVAD(stream);
      
      // ì˜¤ë””ì˜¤ ë ˆë²¨ ë¶„ì„ ì‹œì‘
      startAudioLevelAnalysis(stream);
      
      // VAD ì‹œì‘
      if (vadRef.current) {
        vadRef.current.start();
        console.log('ğŸ§ VAD ë¦¬ìŠ¤ë‹ ì‹œì‘');
      }

      setIsRecording(true);
      
    } catch (err) {
      const errorMessage = err instanceof Error ? err.message : 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜';
      setError('ë…¹ìŒ ì‹œì‘ ì‹¤íŒ¨: ' + errorMessage);
      console.error('Error starting VAD recording:', err);
    }
  }, [onAudioSegment, initializeVAD]);

  const pauseRecording = useCallback(() => {
    // VAD ì™„ì „ ì¤‘ì§€ (destroyí•˜ê³  ì¬ìƒì„±í•˜ì§€ ì•ŠìŒ)
    if (vadRef.current) {
      vadRef.current.pause();
      console.log('â¸ï¸ VAD ì™„ì „ ì¤‘ì§€');
    }

    // ì§„í–‰ ì¤‘ì¸ ë…¹ìŒì´ ìˆìœ¼ë©´ ì¤‘ì§€
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
      mediaRecorderRef.current.stop();
    }

    // ì˜¤ë””ì˜¤ ë ˆë²¨ ë¶„ì„ ì™„ì „ ì¤‘ì§€ (AudioContextë„ ì¼ì‹œì •ì§€)
    stopAudioLevelAnalysis();
    console.log('ğŸ”‡ ì˜¤ë””ì˜¤ ë¶„ì„ ì¤‘ì§€');

    setVadStatus('idle');
  }, [stopAudioLevelAnalysis]);

  const resumeRecording = useCallback(() => {
    // VAD ì¬ê°œ
    if (vadRef.current) {
      vadRef.current.start();
      console.log('â–¶ï¸ VAD ì¬ê°œ');
    }

    // ì˜¤ë””ì˜¤ ë ˆë²¨ ë¶„ì„ ì¬ê°œ (ìŠ¤íŠ¸ë¦¼ì„ ë‹¤ì‹œ ì—°ê²°)
    if (streamRef.current) {
      startAudioLevelAnalysis(streamRef.current);
      console.log('ğŸ¤ ì˜¤ë””ì˜¤ ë¶„ì„ ì¬ê°œ');
    }

    setVadStatus('listening');
  }, [startAudioLevelAnalysis]);

  const stopRecording = useCallback(() => {
    // VAD ì¤‘ì§€
    if (vadRef.current) {
      vadRef.current.pause();
      console.log('ğŸ›‘ VAD ì¤‘ì§€');
    }

    // ì§„í–‰ ì¤‘ì¸ ë…¹ìŒì´ ìˆìœ¼ë©´ ì¤‘ì§€
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
      mediaRecorderRef.current.stop();
    }

    // ì˜¤ë””ì˜¤ ë ˆë²¨ ë¶„ì„ ì™„ì „ ì¤‘ì§€
    stopAudioLevelAnalysis();

    // ìŠ¤íŠ¸ë¦¼ ì •ë¦¬
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
      streamRef.current = null;
    }

    setIsRecording(false);
    setVadStatus('idle');
    segmentCountRef.current = 0;
  }, [stopAudioLevelAnalysis]);

  // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ì •ë¦¬
  useEffect(() => {
    return () => {
      stopAudioLevelAnalysis();
      if (vadRef.current) {
        vadRef.current.destroy();
      }
      if (streamRef.current) {
        streamRef.current.getTracks().forEach(track => track.stop());
      }
    };
  }, [stopAudioLevelAnalysis]);

  return {
    isRecording,
    error,
    vadStatus, // idle, listening, speaking, processing
    segmentCount: segmentCountRef.current,
    audioLevel,
    startRecording,
    stopRecording,
    pauseRecording,
    resumeRecording
  };
};