import { useState, useRef, useCallback, useEffect } from 'react';
import { MicVAD } from '@ricky0123/vad-web';

export const useVADRecording = (onAudioSegment) => {
  const [isRecording, setIsRecording] = useState(false);
  const [error, setError] = useState(null);
  const [vadStatus, setVadStatus] = useState('idle'); // idle, listening, speaking, processing
  
  const vadRef = useRef(null);
  const streamRef = useRef(null);
  const mediaRecorderRef = useRef(null);
  const chunksRef = useRef([]);
  const segmentStartTimeRef = useRef(0);
  const segmentCountRef = useRef(0);

  // VAD ì´ˆê¸°í™”
  const initializeVAD = useCallback(async (stream) => {
    try {
      console.log('VAD ì´ˆê¸°í™” ì¤‘...');
      
      const vad = await MicVAD.new({
        onSpeechStart: () => {
          console.log('ðŸŽ¤ ìŒì„± ì‹œìž‘ ê°ì§€');
          setVadStatus('speaking');
          
          // ìŒì„±ì´ ì‹œìž‘ë˜ë©´ ë…¹ìŒ ì‹œìž‘
          if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'recording') {
            chunksRef.current = [];
            segmentStartTimeRef.current = Date.now();
            segmentCountRef.current++;
            
            console.log(`ì„¸ê·¸ë¨¼íŠ¸ #${segmentCountRef.current} ë…¹ìŒ ì‹œìž‘`);
            mediaRecorderRef.current.start();
          }
        },
        
        onSpeechEnd: (audio) => {
          console.log('ðŸ”‡ ìŒì„± ì¢…ë£Œ ê°ì§€');
          setVadStatus('processing');
          
          // ìŒì„±ì´ ëë‚˜ë©´ ë…¹ìŒ ì¤‘ì§€ ë° STT ì²˜ë¦¬
          if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
            console.log(`ì„¸ê·¸ë¨¼íŠ¸ #${segmentCountRef.current} ë…¹ìŒ ì¢…ë£Œ`);
            mediaRecorderRef.current.stop();
          }
        },
        
        onVADMisfire: () => {
          console.log('âš ï¸ VAD ì˜¤íƒì§€');
          setVadStatus('listening');
        },
        
        // VAD ì„¤ì •
        positiveSpeechThreshold: 0.5,  // ìŒì„± ê°ì§€ ë¯¼ê°ë„
        negativeSpeechThreshold: 0.35, // ìŒì„± ì¢…ë£Œ ë¯¼ê°ë„
        redemptionFrames: 8,           // ìŒì„± ë³µêµ¬ í”„ë ˆìž„
        frameSamples: 1536,            // í”„ë ˆìž„ í¬ê¸°
        preSpeechPadFrames: 1,         // ìŒì„± ì‹œìž‘ ì „ íŒ¨ë”©
        minSpeechFrames: 3,            // ìµœì†Œ ìŒì„± í”„ë ˆìž„
        stream: stream
      });
      
      vadRef.current = vad;
      console.log('âœ… VAD ì´ˆê¸°í™” ì™„ë£Œ');
      
    } catch (err) {
      console.error('VAD ì´ˆê¸°í™” ì‹¤íŒ¨:', err);
      setError('ìŒì„± ê°ì§€ ì´ˆê¸°í™” ì‹¤íŒ¨: ' + err.message);
    }
  }, []);

  const startRecording = useCallback(async () => {
    try {
      setError(null);
      setVadStatus('listening');
      segmentCountRef.current = 0;
      
      // ë§ˆì´í¬ ìŠ¤íŠ¸ë¦¼ íšë“
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          sampleRate: 16000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        }
      });

      streamRef.current = stream;
      
      // MediaRecorder ì„¤ì •
      const mediaRecorder = new MediaRecorder(stream, {
        mimeType: 'audio/webm;codecs=opus'
      });
      
      mediaRecorderRef.current = mediaRecorder;

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          chunksRef.current.push(event.data);
        }
      };

      mediaRecorder.onstop = async () => {
        if (chunksRef.current.length > 0) {
          const blob = new Blob(chunksRef.current, { type: 'audio/webm' });
          const segmentStartTime = segmentStartTimeRef.current;
          
          console.log(`ì„¸ê·¸ë¨¼íŠ¸ #${segmentCountRef.current} STT ì²˜ë¦¬ ìš”ì²­`);
          
          // STT ì²˜ë¦¬
          await onAudioSegment(blob, segmentStartTime);
          
          chunksRef.current = [];
        }
        
        // STT ì²˜ë¦¬ ì™„ë£Œ í›„ ë‹¤ì‹œ ëŒ€ê¸° ìƒíƒœë¡œ
        setVadStatus('listening');
      };

      // VAD ì´ˆê¸°í™” ë° ì‹œìž‘
      await initializeVAD(stream);
      
      // VAD ì‹œìž‘
      if (vadRef.current) {
        vadRef.current.start();
        console.log('ðŸŽ§ VAD ë¦¬ìŠ¤ë‹ ì‹œìž‘');
      }

      setIsRecording(true);
      
    } catch (err) {
      setError('ë…¹ìŒ ì‹œìž‘ ì‹¤íŒ¨: ' + err.message);
      console.error('Error starting VAD recording:', err);
    }
  }, [onAudioSegment, initializeVAD]);

  const stopRecording = useCallback(() => {
    // VAD ì¤‘ì§€
    if (vadRef.current) {
      vadRef.current.pause();
      console.log('ðŸ›‘ VAD ì¤‘ì§€');
    }

    // ì§„í–‰ ì¤‘ì¸ ë…¹ìŒì´ ìžˆìœ¼ë©´ ì¤‘ì§€
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
      mediaRecorderRef.current.stop();
    }

    // ìŠ¤íŠ¸ë¦¼ ì •ë¦¬
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
      streamRef.current = null;
    }

    setIsRecording(false);
    setVadStatus('idle');
    segmentCountRef.current = 0;
  }, []);

  // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ì •ë¦¬
  useEffect(() => {
    return () => {
      if (vadRef.current) {
        vadRef.current.destroy();
      }
      if (streamRef.current) {
        streamRef.current.getTracks().forEach(track => track.stop());
      }
    };
  }, []);

  return {
    isRecording,
    error,
    vadStatus, // idle, listening, speaking, processing
    segmentCount: segmentCountRef.current,
    startRecording,
    stopRecording
  };
};